{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1448e2a9534e04b2cccaf0300eaf35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1610648957158_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-10-9.ec2.internal:20888/proxy/application_1610648957158_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-4-121.ec2.internal:8042/node/containerlogs/container_1610648957158_0002_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Using cached boto3-1.16.54-py2.py3-none-any.whl (130 kB)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/site-packages (from boto3) (0.9.4)\n",
      "Collecting botocore<1.20.0,>=1.19.54\n",
      "  Using cached botocore-1.19.54-py2.py3-none-any.whl (7.2 MB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.54->boto3) (1.26.2)\n",
      "Collecting python-dateutil<3.0.0,>=2.1\n",
      "  Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.54->boto3) (1.13.0)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Using cached s3transfer-0.3.4-py2.py3-none-any.whl (69 kB)\n",
      "Installing collected packages: python-dateutil, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.16.54 botocore-1.19.54 python-dateutil-2.8.1 s3transfer-0.3.4\n",
      "\n",
      "Collecting pandas==1.0.0\n",
      "  Using cached pandas-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /mnt/tmp/1610649786483-0/lib/python3.6/site-packages (from pandas==1.0.0) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.6/site-packages (from pandas==1.0.0) (1.14.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas==1.0.0) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas==1.0.0) (1.13.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.0.0\n",
      "\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/site-packages (2.25.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests) (1.26.2)\n",
      "\n",
      "Collecting s3fs\n",
      "  Using cached s3fs-0.5.1-py3-none-any.whl (21 kB)\n",
      "Collecting aiobotocore>=1.0.1\n",
      "  Using cached aiobotocore-1.2.0-py3-none-any.whl\n",
      "Collecting aiohttp>=3.3.1\n",
      "  Using cached aiohttp-3.7.3-cp36-cp36m-manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting aioitertools>=0.5.1\n",
      "  Using cached aioitertools-0.7.1-py3-none-any.whl (20 kB)\n",
      "Collecting async-timeout<4.0,>=3.0\n",
      "  Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
      "Collecting botocore<1.19.53,>=1.19.52\n",
      "  Using cached botocore-1.19.52-py2.py3-none-any.whl (7.2 MB)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (0.9.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.6/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /mnt/tmp/1610649786483-0/lib/python3.6/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (2.8.1)\n",
      "Collecting chardet<4.0,>=2.0\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting fsspec>=0.8.0\n",
      "  Using cached fsspec-0.8.5-py3-none-any.whl (98 kB)\n",
      "Collecting idna-ssl>=1.0\n",
      "  Using cached idna_ssl-1.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.6/site-packages (from idna-ssl>=1.0->aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (2.10)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-5.1.0-cp36-cp36m-manylinux2014_x86_64.whl (141 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (1.13.0)\n",
      "Collecting typing-extensions>=3.6.5\n",
      "  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting wrapt>=1.10.10\n",
      "  Using cached wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.6.3-cp36-cp36m-manylinux2014_x86_64.whl (293 kB)\n",
      "Installing collected packages: typing-extensions, multidict, yarl, idna-ssl, chardet, attrs, async-timeout, wrapt, botocore, aioitertools, aiohttp, fsspec, aiobotocore, s3fs\n",
      "  Attempting uninstall: chardet\n",
      "    Found existing installation: chardet 4.0.0\n",
      "    Not uninstalling chardet at /usr/local/lib/python3.6/site-packages, outside environment /tmp/1610649786483-0\n",
      "    Can't uninstall 'chardet'. No files were found to uninstall.\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.19.54\n",
      "    Uninstalling botocore-1.19.54:\n",
      "      Successfully uninstalled botocore-1.19.54\n",
      "Successfully installed aiobotocore-1.2.0 aiohttp-3.7.3 aioitertools-0.7.1 async-timeout-3.0.1 attrs-20.3.0 botocore-1.19.52 chardet-3.0.4 fsspec-0.8.5 idna-ssl-1.1.0 multidict-5.1.0 s3fs-0.5.1 typing-extensions-3.7.4.3 wrapt-1.12.1 yarl-1.6.3\n",
      "\n",
      "Requirement already satisfied: fsspec in /mnt/tmp/1610649786483-0/lib/python3.6/site-packages (0.8.5)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "boto3 1.16.54 requires botocore<1.20.0,>=1.19.54, but you have botocore 1.19.52 which is incompatible."
     ]
    }
   ],
   "source": [
    "# Install libraries within the notebook scope\n",
    "sc.install_pypi_package(\"boto3\")\n",
    "sc.install_pypi_package(\"pandas==1.0.0\")\n",
    "sc.install_pypi_package(\"requests\")\n",
    "sc.install_pypi_package(\"s3fs\")\n",
    "sc.install_pypi_package(\"fsspec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from datetime import datetime\n",
    "import fsspec\n",
    "import pandas as pd\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql import functions as f, types as t, Window\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "import s3fs\n",
    "import subprocess\n",
    "import timeit\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Removes truncation of columns, column values in Pandas\n",
    "# by default\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "# Monkey patching the DataFrame transform method for Spark 2.4\n",
    "# This is available by default in Spark 3.0\n",
    "def transform(self, f):\n",
    "    return f(self)\n",
    "DataFrame.transform = transform\n",
    "\n",
    "# Override the timeit template to return the command's\n",
    "# return value in addition to the time\n",
    "# Reference: https://stackoverflow.com/questions/24812253/how-can-i-capture-return-value-with-python-timeit-module\n",
    "timeit.template = \"\"\"\n",
    "def inner(_it, _timer{init}):\n",
    "    {setup}\n",
    "    _t0 = _timer()\n",
    "    for _i in _it:\n",
    "        retval = {stmt}\n",
    "    _t1 = _timer()\n",
    "    return _t1 - _t0, retval\n",
    "\"\"\"\n",
    "\n",
    "def shell_cmd(cmd):\n",
    "    \"\"\"\n",
    "    Wrapper for running shell commands and printing the output\n",
    "    Some helpful recipes:\n",
    "    - List files on hdfs: shell_cmd(\"hdfs dfs -ls hdfs:///tmp/data/\")\n",
    "    - Remove files from hdfs: shell_cmd(\"hdfs dfs -rm -r hdfs:///tmp/data/test_pyspark\")\n",
    "    \"\"\"\n",
    "    for line in subprocess.check_output(cmd, shell=True).split(b'\\n'):\n",
    "        print(line)\n",
    "\n",
    "def timer_method(cmd):\n",
    "    \"\"\"\n",
    "    Wrapper for timeit that returns the value of a function and its runtime\n",
    "    To use, pass a string of the function you wish to time\n",
    "    Example: \n",
    "     run_time, result = timer_method(\"myfunction(arg1, arg2)\")\n",
    "    \"\"\"\n",
    "    # Setting globals = globals() enables the timeit function\n",
    "    # to return the value generated by cmd\n",
    "    return timeit.timeit(cmd, number=1, globals = globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set your s3 bucket name\n",
    "This should be data-scale-oreilly-{your name}   \n",
    "If you dont remember check the [S3 console](https://s3.console.aws.amazon.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_BUCKET_NAME = \"data-scale-oreilly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesting from an S3 bucket - NYC Taxi Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "* Taxi data \n",
    "* Data dictionaries \n",
    "* Taxi zone lookup table\n",
    "\n",
    "Data ingestion has the ultimate goal of collecting, aggregating, and surfacing data for a specific purpose; an analysis, an API, a dashboard, etc. Think about how you might use the taxi data to answer the following questions:\n",
    "\n",
    "1. Which borough is the most popular pickup or drop off spot?\n",
    "1. Are green taxis more popular for trips within the same borough vs yellow taxis?\n",
    "1. Build a recommendation engine that predicts surge pricing for a given time of day based on historical data  \n",
    "\n",
    "With this in mind, lets work through bringing this data onto the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, if you copy the link from the taxi data website you will see:\n",
    "# https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-01.csv\n",
    "# Two things - first, the portion of the URL following \"aws.com\" is the \n",
    "# bucket name. Second, in \"trip+data\" the \"+\" is a space\n",
    "taxi_data_path = \"s3://nyc-tlc/trip data/yellow_tripdata_2020-01.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas uses s3fs to read_csv from s3:\n",
    "pd_df_taxi= pd.read_csv(taxi_data_path, keep_default_na=False)\n",
    "print(pd_df_taxi.head())\n",
    "pd_df_taxi.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reference, look at the Spark DataFrameReader, csv:\n",
    "# https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html\n",
    "ps_df_taxi = spark.read.option('header', True).option('inferSchema', True).csv(taxi_data_path)\n",
    "ps_df_taxi.show()\n",
    "ps_df_taxi.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Talk through ingest practices around retaining original data vs augmenting\n",
    "# For example, we may want to keep the data in its default format so we can\n",
    "# refer back to it if there are bugs in our data ingestion code\n",
    "ps_df_taxi.write.option(\"header\", True).csv(\"hdfs:///tmp/input/taxi_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discuss how spark writes files out\n",
    "shell_cmd(\"hdfs dfs -ls hdfs:///tmp/input/taxi_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_subset = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID', 'fare_amount', 'tip_amount']\n",
    "ps_df_taxi.select(*column_subset).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_df_taxi.select(*column_subset).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.1 - Write an ingestion function that does the following:\n",
    "Given a file path to a taxi data csv (i.e. s3://nyc-tlc/trip data/green_tripdata_2020-01.csv) create a function that does the following:\n",
    "1. Read the file into a Spark dataframe\n",
    "1. Limit to the `column_subset` columns\n",
    "1. Write the data as json to hdfs in append mode to `hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json`\n",
    "\n",
    "Function signature:  \n",
    "`def ingest_taxi_data(file_name)`\n",
    "\n",
    "See the subsequent cell for more info on how the `ingest_taxi_data` function will be used   \n",
    "Reference: https://spark.apache.org/docs/2.4.5/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter  \n",
    "\n",
    "When you're done, run the next 2 cells to ingest several taxi data files and examine the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_taxi_data(file_name):\n",
    "    # Enclosing code in () allows multi line\n",
    "    (spark\n",
    "         .read\n",
    "         .option('header', True)\n",
    "         .option(\"inferSchema\", True)\n",
    "         .csv(file_name)\n",
    "         .select(*column_subset)           \n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the ingest for several files\n",
    "taxi_data_prefix = \"s3://nyc-tlc/trip data\"\n",
    "taxi_data_files = [\"yellow_tripdata_2019-01.csv\", \"yellow_tripdata_2018-01.csv\"]\n",
    "for file_name in taxi_data_files: \n",
    "    taxi_data_path = f\"{taxi_data_prefix}/{file_name}\"\n",
    "    ingest_taxi_data(taxi_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How did the types fare in this conversion?\n",
    "# Turns out its a bug! \n",
    "# https://issues.apache.org/jira/browse/SPARK-26325\n",
    "# https://stackoverflow.com/questions/53697388/interpret-timestamp-fields-in-spark-while-reading-json\n",
    "df = spark.read.json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "df.printSchema()\n",
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming data types \n",
    "\n",
    "Available pyspark types are listed in the pyspark.sql.types module https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#module-pyspark.sql.types\n",
    "\n",
    "pyspark.types is imported as t, so to apply the IntegerType use t.IntegerType()\n",
    "\n",
    "For pandas, see the following resources on converting types https://stackoverflow.com/questions/15891038/change-column-type-in-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark\n",
    "(df.select(\"tpep_dropoff_datetime\")\n",
    " .withColumn(\"tpep_dropoff_datetime\", f.col(\"tpep_dropoff_datetime\").cast(t.TimestampType()))\n",
    ").dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casting pandas columns to a type - this will give an error on empty cells\n",
    "(pd_df_taxi[[*column_subset]]\n",
    "        .astype({'passenger_count': 'Int64'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert to Integer using pandas, we have to first deal with the null values\n",
    "# to_numeric with 'coerce' will fill invalid integer values with np.NaN\n",
    "# the Int64 type in later versions of pandas will convert np.NaN to a nullable\n",
    "# integer type: https://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html\n",
    "pd.to_numeric(pd_df_taxi.passenger_count, errors='coerce').astype('Int64').dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified taxi_data_ingest with transformed timestamps\n",
    "def ingest_taxi_data(file_name):\n",
    "    # Enclosing code in () allows multi line\n",
    "    (spark\n",
    "         .read\n",
    "         .option('header', True)\n",
    "         .option(\"inferSchema\", True)\n",
    "         .csv(file_name)\n",
    "         .select(*column_subset)\n",
    "         .withColumn(\"tpep_pickup_date\", f.col(\"tpep_pickup_datetime\").cast(t.DateType()))\n",
    "         .withColumn(\"tpep_dropoff_date\", f.col(\"tpep_dropoff_datetime\").cast(t.DateType()))\n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove previous data\n",
    "shell_cmd(\"hdfs dfs -rm -r hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data_prefix = \"s3://nyc-tlc/trip data\"\n",
    "taxi_data_files = [\"yellow_tripdata_2019-01.csv\", \"yellow_tripdata_2018-01.csv\"]\n",
    "for file_name in taxi_data_files: \n",
    "    taxi_data_path = f\"{taxi_data_prefix}/{file_name}\"\n",
    "    ingest_taxi_data(taxi_data_path)\n",
    "    \n",
    "df = spark.read.json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing ingestion code\n",
    "\n",
    "The `ingest_taxi_data` method is not well structured for testing:\n",
    "* Writes to the file system\n",
    "* Requires an input file to test\n",
    "* What other shortcomings?\n",
    "\n",
    "To make this code more testable, split out the transformation logic so it can be unit tested.  \n",
    "Definining a transformation function that takes a dataframe and returns a dataframe provides a better interface for unit testing, and a more extensible structure in case we need to add more dataframe functions before or after the transformation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_taxi_data(df):\n",
    "    return (df\n",
    "            .withColumn(\"tpep_pickup_date\", f.col(\"tpep_pickup_datetime\").cast(t.DateType()))\n",
    "            .withColumn(\"tpep_dropoff_date\", f.col(\"tpep_dropoff_datetime\").cast(t.DateType()))\n",
    "           )\n",
    "\n",
    "# Option 1\n",
    "def ingest_taxi_data_method(file_name):\n",
    "    df_input = (spark\n",
    "         .read\n",
    "         .option('header', True).csv(taxi_data_path)\n",
    "         .select(*column_subset))\n",
    "    \n",
    "    (transform_taxi_data(df_input)\n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )\n",
    "\n",
    "# Option 2\n",
    "def ingest_taxi_data_transform(file_name):\n",
    "    # Requires patching of Dataframe.transform method in Spark 2.4, but available natively\n",
    "    # in Spark 3.0 https://mungingdata.com/pyspark/chaining-dataframe-transformations/\n",
    "    df_input = (spark\n",
    "         .read\n",
    "         .option('header', True).csv(taxi_data_path)\n",
    "         .select(*column_subset)\n",
    "         .transform(transform_taxi_data)\n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    \"{'tpep_pickup_datetime': '2020-05-23', 'tpep_dropoff_datetime': '2020-05-23'}\",\n",
    "    \"{'tpep_pickup_datetime': '2020-10-01', 'tpep_dropoff_datetime': '2020-10-01'}\",\n",
    "    \"{'tpep_pickup_datetime': '2020-02-02', 'tpep_dropoff_datetime': '2020-02-03'}\"\n",
    "]\n",
    "expected_types = {'tpep_dropoff_date': 'date', 'tpep_pickup_date': 'date', 'tpep_pickup_datetime':'string', 'tpep_dropoff_datetime':'string'}\n",
    "\n",
    "test_df = spark.read.json(sc.parallelize(test_data))\n",
    "print(test_df.dtypes)\n",
    "test = transform_taxi_data(test_df)\n",
    "test_types = {item[0]:item[1] for item in test.dtypes}\n",
    "\n",
    "print(expected_types == test_types)\n",
    "\n",
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets try running the ingestion code on the other taxi data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try using the ingest code we created for yellow taxi for all the taxis\n",
    "# This will fail because the datetime fields have different names across different servcies\n",
    "\n",
    "taxi_data_prefix = \"s3://nyc-tlc/trip data/\"\n",
    "taxi_data_files = [\"green_tripdata_2020-01.csv\", \"fhv_tripdata_2020-01.csv\", \"fhvhv_tripdata_2020-01.csv\"]\n",
    "for file_name in taxi_data_files: \n",
    "    taxi_data_path = f\"{taxi_data_prefix}{file_name}\"\n",
    "    ingest_taxi_data_transform(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can we ingest all taxi services AND be able to tell them apart?\n",
    "\n",
    "Taxi file names: \n",
    "* yellow_tripdata_2020-01.csv\n",
    "* green_tripdata_2020-01.csv\n",
    "* fhv_tripdata_2020-01.csv\n",
    "* fhvhv_tripdata_2020-01.csv\n",
    "\n",
    "The file name provides information including:\n",
    "* Service type (yellow, green, etc)\n",
    "* File date\n",
    "\n",
    "We want to augment the taxi data with this information so we can refer back to it in analysis.\n",
    "\n",
    "Is there other data we might want to augment the raw data with? Some things to consider:\n",
    "* Additional fields that could help with analysis\n",
    "* Metadata, such as when the record was last updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using matched groups, we can extract information from the taxi file names\n",
    "# i.e. yellow_tripdata_2020-01.csv\n",
    "TAXI_DATA_PATTERN = \"(?P<service>[a-zA-Z0-9]+)_tripdata_(?P<year>[0-9]{4})-(?P<month>[0-9]{2}).csv\"\n",
    "\n",
    "def extract_file_info(file_name):\n",
    "    # Returns (service, year, month) given a taxi file name\n",
    "    m = re.match(TAXI_DATA_PATTERN, file_name)\n",
    "    if m is not None:\n",
    "        return (m.group(1), m.group(2), m.group(3))\n",
    "    \n",
    "extract_file_info(\"yellow_tripdata_2020-01.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.2 - Ingesting multiple taxi service types\n",
    "\n",
    "See the [Taxi data website](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) for reference\n",
    "\n",
    "Using the template in the next cell, create the following functions:\n",
    "* Service specific transformations to match the schema below\n",
    "* A general transformation function to apply metadata and other common transformations\n",
    "\n",
    "Schema:\n",
    "\n",
    "* pickup_datetime Timestamp\n",
    "* dropoff_datetime Timestamp\n",
    "* pickup_date Date\n",
    "* dropoff_date Date\n",
    "* passenger_count Integer\n",
    "* fare_amount Float\n",
    "* tip_amount Float\n",
    "* trip_distance Float\n",
    "* PULocationID Integer\n",
    "* DOLocationID Integer\n",
    "\n",
    "Metadata fields:  explore `f.lit` to add these columns\n",
    "* service\n",
    "* year\n",
    "* month\n",
    "\n",
    "Refer to `ingest_taxi_data_multi_service` to see how these functions will be used    \n",
    "\n",
    "You may find some helpful info here: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_yellow_taxi(df):\n",
    "    subset = ['pickup_datetime', 'dropoff_datetime', 'passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID', 'fare_amount', 'tip_amount']\n",
    "    return (df.withColumnRenamed(\"tpep_pickup_datetime\", \"pickup_datetime\")\n",
    "        .withColumnRenamed(\"tpep_dropoff_datetime\", \"dropoff_datetime\")\n",
    "        .select(*subset)\n",
    "        .withColumn(\"dropoff_date\", f.col(\"dropoff_datetime\").cast(t.DateType()))\n",
    "        .withColumn(\"pickup_date\", f.col(\"pickup_datetime\").cast(t.DateType()))\n",
    "\n",
    "        )\n",
    "        \n",
    "def transform_green_taxi(df):\n",
    "    subset = ['pickup_datetime', 'dropoff_datetime', 'passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID', 'fare_amount', 'tip_amount']\n",
    "    return (df.withColumnRenamed(\"lpep_pickup_datetime\", \"pickup_datetime\")\n",
    "        .withColumnRenamed(\"lpep_dropoff_datetime\", \"dropoff_datetime\")\n",
    "        .select(*subset)\n",
    "        .withColumn(\"dropoff_date\", f.col(\"dropoff_datetime\").cast(t.DateType()))\n",
    "        .withColumn(\"pickup_date\", f.col(\"pickup_datetime\").cast(t.DateType()))\n",
    "        )\n",
    "\n",
    "def transform_fhv(df):\n",
    "    return df.select(*[\"pickup_datetime\", \"dropoff_datetime\", \"PULocationID\", \"DOLocationID\"])\n",
    "\n",
    "def transform_all(df, service, year, month):\n",
    "    return (df.withColumn(\"service\", f.lit(service))\n",
    "         .withColumn(\"year\", f.lit(year))\n",
    "         .withColumn(\"month\", f.lit(month))\n",
    "         .withColumn(\"dropoff_date\", f.col(\"dropoff_datetime\").cast(t.DateType()))\n",
    "         .withColumn(\"pickup_date\", f.col(\"pickup_datetime\").cast(t.DateType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_taxi_data_multi_service(file_name, ingested_on):\n",
    "    print(f\"Processing {file_name}\")\n",
    "    (service, year, month) = extract_file_info(Path(file_name).name)\n",
    "    input_df = spark.read.option('header', True).option('inferSchema', True).csv(file_name)\n",
    "    \n",
    "    if service == 'yellow':\n",
    "        df_transform = transform_yellow_taxi(input_df)\n",
    "    elif service == 'green':\n",
    "        df_transform = transform_green_taxi(input_df)\n",
    "    else:\n",
    "        # FHV. What happens if there are more taxi services added?\n",
    "        df_transform = transform_fhv(input_df)\n",
    "        \n",
    "    print(df_transform.dtypes)\n",
    "\n",
    "    (transform_all(df_transform, service, year, month)\n",
    "         .withColumn(\"ingested_on\", f.lit(ingest_timestamp))\n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_cmd(\"hdfs dfs -rm -r hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_timestamp = datetime.strftime(datetime.now(), \"%Y-%m-%d %H:%M:%S%z\")\n",
    "taxi_data_prefix = \"s3://nyc-tlc/trip data/\"\n",
    "taxi_data_files = [\"yellow_tripdata_2020-01.csv\"]#, \"green_tripdata_2020-01.csv\", \"fhv_tripdata_2020-01.csv\", \"fhvhv_tripdata_2020-01.csv\"]\n",
    "for file_name in taxi_data_files: \n",
    "    taxi_data_path = f\"{taxi_data_prefix}{file_name}\"\n",
    "    ingest_taxi_data_multi_service(taxi_data_path, ingest_timestamp)\n",
    "    \n",
    "df_taxi_output = spark.read.json(\"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json\")\n",
    "df_taxi_output.show(5)\n",
    "df_taxi_output.groupby(\"service\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling bad data\n",
    "How to design for the inevitability of bad data  \n",
    "Reference: https://blog.knoldus.com/apache-spark-handle-corrupt-bad-records/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_data = [\n",
    "    \"{'pickup_datetime': '2020-05-23 21:05:23', 'fare_amount': '0.05'}\",\n",
    "    \"{'pickup_datetime': '2020-05-23 08:05:23', 'fare_amount': '10.05'}\",\n",
    "    \"{'pickup_datetime': '2020-05-23 21:05:23', 'fare_amount}\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_df = spark.read.json(sc.parallelize(bad_data), mode=\"PERMISSIVE\", columnNameOfCorruptRecord=\"_corrupt_record\")\n",
    "corrupt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_df = spark.read.json(sc.parallelize(bad_data), mode=\"DROPMALFORMED\", columnNameOfCorruptRecord=\"_corrupt_record\")\n",
    "corrupt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_df = spark.read.json(sc.parallelize(bad_data), mode=\"FAILFAST\", columnNameOfCorruptRecord=\"_corrupt_record\")\n",
    "corrupt_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.3 - Write an ingestion for the taxi zone lookup\n",
    "File location - Yes, there is a space between taxi and the '_'  \n",
    "\n",
    "s3://nyc-tlc/misc/taxi _zone_lookup.csv\n",
    "\n",
    "`def ingest_taxi_lookup():`\n",
    "1. Read taxi lookup data, ensuring data types are correct\n",
    "1. Add relevant metadata\n",
    "1. Save to hdfs:///tmp/data/nyc-taxi/zone-lookup/output/section2/json\n",
    "1. What write mode should be used?\n",
    "\n",
    "Refer back to Taxi Data page for more info: https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taxi_zone_transform(df):\n",
    "    return df.withColumn(\"ingested_on\", f.lit(ingest_timestamp))\n",
    "\n",
    "def ingest_taxi_lookup(ingest_timestamp):\n",
    "    (spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"s3://nyc-tlc/misc/taxi _zone_lookup.csv\")\n",
    "    .transform(taxi_zone_transform)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .json(\"hdfs:///tmp/data/nyc-taxi/zone-lookup/output/section2/json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_timestamp = datetime.strftime(datetime.now(), \"%Y-%m-%d %H:%M:%S%z\")\n",
    "print(timer_method(\"ingest_taxi_lookup(ingest_timestamp)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.4 - Case Study 1: Month over month, get the total count of of pickups per borough\n",
    "#### Do not blindly run hese cells, you can bork your cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxiPath = \"hdfs:///tmp/data/nyc-taxi/taxi-data/output/section2/json/\"\n",
    "taxiLookupPath = \"hdfs:///tmp/data/nyc-taxi/zone-lookup/output/section2/json/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join boroughs\n",
    "# Expected error cartesian join. most likely a carryover bug from 2.0\n",
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", \"false\") #<-- default\n",
    "df_taxi = spark.read.json(taxiPath)\n",
    "df_taxi_lookup = spark.read.json(taxiLookupPath)\n",
    "\n",
    "taxi_filtered = (df_taxi\n",
    " .filter(df_taxi.pickup_datetime.isNotNull())\n",
    " .filter(df_taxi.dropoff_datetime.isNotNull()))\n",
    "taxi_pu = (taxi_filtered\n",
    ".join(df_taxi_lookup\n",
    "       .select(\"LocationID\", \"Borough\")\n",
    "       .withColumnRenamed(\"Borough\", \"PUBorough\"), \n",
    "       df_taxi_lookup.LocationID == df_taxi.PULocationID))\n",
    "taxi = (taxi_pu.join(df_taxi_lookup\n",
    "       .select(\"LocationID\", \"Borough\")\n",
    "       .withColumnRenamed(\"Borough\", \"DOBorough\"), \n",
    "       df_taxi_lookup.LocationID == taxi_pu.DOLocationID))\n",
    "taxi_pu.show()\n",
    "taxi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")\n",
    "df_taxi = spark.read.json(taxiPath)\n",
    "df_taxi_lookup = spark.read.json(taxiLookupPath)\n",
    "\n",
    "taxi_filtered = (df_taxi\n",
    " .filter(df_taxi.pickup_datetime.isNotNull())\n",
    " .filter(df_taxi.dropoff_datetime.isNotNull()))\n",
    "taxi_pu = (taxi_filtered\n",
    ".join(df_taxi_lookup\n",
    "       .select(\"LocationID\", \"Borough\")\n",
    "       .withColumnRenamed(\"Borough\", \"PUBorough\"), \n",
    "       df_taxi_lookup.LocationID == df_taxi.PULocationID))\n",
    "taxi = (taxi_pu.join(df_taxi_lookup\n",
    "       .select(\"LocationID\", \"Borough\")\n",
    "       .withColumnRenamed(\"Borough\", \"DOBorough\"), \n",
    "       df_taxi_lookup.LocationID == taxi_pu.DOLocationID))\n",
    "taxi_pu.show()\n",
    "taxi.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monthly_totals_pyspark(taxiPath, taxiLookupPath):\n",
    "    taxi = spark.read.json(taxiPath)\n",
    "    taxi_lookup = spark.read.json(taxiLookupPath)\n",
    "    taxi_filtered = (taxi\n",
    "     .filter(taxi.pickup_datetime.isNotNull())\n",
    "     .filter(taxi.dropoff_datetime.isNotNull()))\n",
    "                     \n",
    "    groupDF = taxi_filtered.join(taxi_lookup, taxi_filtered.PULocationID == taxi_lookup.LocationID)\n",
    "    groupDF.select(\"ingested_on\").show() # expected error\n",
    "    return groupDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timer_method(\"get_monthly_totals_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monthly_totals_pandas(taxiPath, taxiLookupPath):\n",
    "    taxi = pd.read_json(taxiPath)\n",
    "    taxi_lookup = pd.read_json(taxiLookupPath)\n",
    "    taxi_filtered = tax.dropna(subset=['pickup_datetime', 'dropoff_datetime'])\n",
    "    \n",
    "    groupDF = taxi_filtered.join(taxi_lookup.set_index('LocationID'), on='PULocationID')\n",
    "    groupDF['pickup_month'] = pd.to_datetime(groupDF['pickup_datetime'], format='%m%Y')\n",
    "    groupDF = groupDF.groupby('pickup_month', 'borough').agg('count').sort_values(by=['count', 'borough'], ascending=[False, True])\n",
    "    groupDF\n",
    "    return groupDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timer_method(\"get_monthly_totals_pandas(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monthly_totals_pandas(taxiPath, taxiLookupPath):\n",
    "    taxiPySpark = spark.read.json(taxiPath)\n",
    "    taxiLookupPySpark = spark.read.json(taxiLookupPath)\n",
    "    \n",
    "    taxi = taxiPySpark.toPandas()\n",
    "    taxiLookup = taxiLookupPySpark.toPandas()\n",
    "    taxiFiltered = taxi.dropna(subset=['pickup_datetime', 'dropoff_datetime'])\n",
    "    \n",
    "    groupDF = taxiFiltered.join(taxiLookup[[\"Borough\", \"LocationID\"]].set_index('LocationID'), on='PULocationID')\n",
    "    \n",
    "    groupDF['pickup_month'] = pd.to_datetime(groupDF['pickup_datetime']).dt.strftime('%Y%m')\n",
    "    returnGroupDF = groupDF.groupby(['pickup_month', 'Borough']).size().reset_index(name='count').sort_values(by=['pickup_month', 'count', 'Borough'], ascending=[False, False, True])\n",
    "    return returnGroupDF\n",
    "\n",
    "def get_monthly_totals_pyspark(taxiPath, taxiLookupPath):\n",
    "    taxi = spark.read.json(taxiPath)\n",
    "    taxiLookup = spark.read.json(taxiLookupPath)\n",
    "    \n",
    "    taxiFiltered = (taxi\n",
    "     .filter(taxi.pickup_datetime.isNotNull())\n",
    "     .filter(taxi.dropoff_datetime.isNotNull()))\n",
    "                     \n",
    "    groupDF = taxiFiltered.join(taxiLookup.select(\"Borough\", \"LocationID\"), taxiFiltered.PULocationID == taxiLookup.LocationID)\n",
    "    groupDF = groupDF.withColumn(\"pickup_month\", f.date_format(\"pickup_datetime\", \"yyyyMM\"))\n",
    "    groupDF = groupDF.groupBy(\"pickup_month\", \"borough\").count().orderBy(f.desc(\"pickup_month\"), f.desc(\"count\"), \"borough\")\n",
    "    groupDF.show()\n",
    "    return groupDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this command with the original cluster size, will crash the cluster\n",
    "# All functions utilizing pandas from this command forward, need an upscaled driver node\n",
    "print(timer_method(\"get_monthly_totals_pandas(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timer_method(\"get_monthly_totals_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Expected error for maxResultSize: This won't work. Could try the subsequent cells\n",
    "## Those restart the state of the notebook and don't work as expected\n",
    "## Need to restart the cluster and edit the Software config with: [{\"classification\":\"spark-defaults\", \"properties\":{\"spark.driver.maxResultSize\":\"5G\", \"spark.ui.killEnabled\":\"true\"}, \"configurations\":[]}]\n",
    "## Then need to reun the taxi and taxi lookup ingests\n",
    "## Run -> Run All Above Selected Cell\n",
    "## Second expected error for {\"msg\":\"requirement failed: Session isn't active.\"} and will hang. Driver node ran out of mem. Will need to go and upscale\n",
    "print(spark.conf.get('spark.driver.maxResultSize'))\n",
    "spark.conf.set(\"spark.driver.maxResultSize\", \"5G\")\n",
    "print(spark.conf.get('spark.driver.maxResultSize'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\"conf\":{\"spark.driver.maxResultSize\":\"5G\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monthly_totals_concat_pandas(taxiPath, taxiLookupPath):\n",
    "    taxiPySpark = spark.read.json(taxiPath)\n",
    "    taxiLookupPySpark = spark.read.json(taxiLookupPath)\n",
    "    \n",
    "    taxi = taxiPySpark.toPandas()\n",
    "    taxiLookup = taxiLookupPySpark.toPandas()\n",
    "    taxiFiltered = taxi.dropna(subset=['pickup_datetime', 'dropoff_datetime'])\n",
    "    \n",
    "    groupDF = taxiFiltered.join(taxiLookup[[\"Borough\", \"LocationID\"]].set_index('LocationID'), on='PULocationID')\n",
    "    groupDF['pickup_month'] = groupDF['year'] + groupDF['month']\n",
    "    groupDF = groupDF.groupby(['pickup_month', 'Borough']).size().reset_index(name='count').sort_values(by=['pickup_month', 'count', 'Borough'], ascending=[False, False, True])\n",
    "    return groupDF\n",
    "    \n",
    "def get_monthly_totals_concat_pyspark(taxiPath, taxiLookupPath):\n",
    "    taxi = spark.read.json(taxiPath)\n",
    "    taxiLookup = spark.read.json(taxiLookupPath)\n",
    "    taxiFiltered = (taxi\n",
    "     .filter(taxi.pickup_datetime.isNotNull())\n",
    "     .filter(taxi.dropoff_datetime.isNotNull()))\n",
    "        \n",
    "    groupDF = taxiFiltered.join(taxiLookup, taxiFiltered.PULocationID == taxiLookup.LocationID)\n",
    "    groupDF = groupDF.withColumn(\"pickup_month\", f.concat(\"year\", \"month\")).select(\"pickup_datetime\", \"borough\", \"pickup_month\")\n",
    "    groupDF = groupDF.groupBy(\"pickup_month\", \"borough\").count().orderBy(f.desc(\"pickup_month\"), f.desc(\"count\"), \"borough\")\n",
    "    groupDF.show()\n",
    "    return groupDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timer_method(\"get_monthly_totals_concat_pandas(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timer_method(\"get_monthly_totals_concat_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2.5 - Case Study 2: Month over month, get the borough with the most amount of pickups per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_pickups_per_month_pandas(taxiPath, taxiLookupPath):\n",
    "    inputDF = get_monthly_totals_pandas(taxiPath, taxiLookupPath)\n",
    "    firstDF = inputDF.groupby(\"pickup_month\").head(1).reset_index(drop=True)#.first()#sort_values(by=['pickup_month', 'count'], ascending=[True, False]).head(1).reset_index(drop=True)\n",
    "    firstDF\n",
    "    return firstDF\n",
    "\n",
    "def get_most_pickups_per_month_pyspark(taxiPath, taxiLookupPath):\n",
    "    inputDF = get_monthly_totals_pyspark(taxiPath, taxiLookupPath)\n",
    "    firstDF = inputDF.orderBy(f.desc(\"pickup_month\"), f.desc(\"count\")).groupBy(\"pickup_month\").agg(f.first(\"borough\")).orderBy(f.desc(\"pickup_month\"))\n",
    "    firstDF.explain()\n",
    "    firstDF.show()\n",
    "    return firstDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timer_method(\"get_most_pickups_per_month_pandas(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timer_method(\"get_most_pickups_per_month_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_pickups_per_month_window_pyspark(taxiPath, taxiLookupPath):\n",
    "    from pyspark.sql import Window\n",
    "    inputDF = get_monthly_totals_pyspark(taxiPath, taxiLookupPath)\n",
    "    win = Window.partitionBy(\"pickup_month\").orderBy(f.desc(\"count\"))\n",
    "    firstDF = inputDF.withColumn(\"row_num\", f.row_number().over(win)).where(\"row_num == 1\")\n",
    "    firstDF = firstDF.orderBy(f.desc(\"pickup_month\"))\n",
    "    firstDF.explain()\n",
    "    firstDF.show(firstDF.count())\n",
    "    return firstDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timer_method(\"get_most_pickups_per_month_window_pyspark(taxiPath, taxiLookupPath)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab - 2.6 Run and time the overall pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset notebook kernel\n",
    "def ingest_main():\n",
    "    ingest_taxi_data_multi_service(\"s3://nyc-tlc/trip data/yellow_tripdata_2020-01.csv\")\n",
    "    ingest_taxi_lookup(\"s3://nyc-tlc/misc/taxi _zone_lookup.csv\")\n",
    "    get_most_pickups_per_month_window_pyspark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timer_method(\"ingest_main()\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
