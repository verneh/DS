{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cocl.us/Data_Science_with_Scalla_top\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/SC0103EN/adds/Data_Science_with_Scalla_notebook_top.png\" width = 750, align = \"center\"></a>\n",
    " <br/>\n",
    "<a><img src=\"https://ibm.box.com/shared/static/ugcqz6ohbvff804xp84y4kqnvvk3bq1g.png\" width=\"200\" align=\"center\"></a>\"\n",
    "\n",
    "# Linear Methods\n",
    "\n",
    "\n",
    "After completing this lesson you should be able to:\n",
    "\n",
    "* Understand the Pipeline API for Logistic Regression and Linear Least Squares\n",
    "* Perform classification with Logistic Regression\n",
    "* Perform regression with Linear Least Squares\n",
    "* Use regularization with Logistic Regression and Linear Least Squares\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "* Widely used to predict binary responses\n",
    "* Can be generalized into multinomial logistic regression\n",
    "\n",
    "The benefits of Logistic Regression are:\n",
    "\n",
    "* there are no tuning parameters\n",
    "* the prediction equation is simple and easy to implement\n",
    "\n",
    "## Continuing from previous example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@2d300341\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.util.MLUtils.{convertVectorColumnsFromML=>fromML, convertVectorColumnsToML=>toML}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@2d300341"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "import org.apache.spark.mllib.util.MLUtils.{\n",
    "  convertVectorColumnsFromML => fromML,\n",
    "  convertVectorColumnsToML => toML\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data = [label: double, features: vector]\n",
       "splitData = Array([label: double, features: vector], [label: double, features: vector])\n",
       "trainingData = [label: double, features: vector]\n",
       "testData = [label: double, features: vector]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[label: double, features: vector]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.util.MLUtils\n",
    " \n",
    "val data = toML(MLUtils.loadLibSVMFile(sc, \"/resources/data/sample_libsvm_data.txt\").toDF())\n",
    "\n",
    "val splitData = data.randomSplit(Array(0.7, 0.3))\n",
    "val trainingData = toML(splitData(0))\n",
    "val testData = toML(splitData(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: (692,[235,243,244,263,272,273,291,300,301,328,351,378,379,385,406,407,428,429,433,434,455,456,461,462,483,484,489,490,511,512,517,539,540,568],[-6.005426723435747E-5,-6.725695887296101E-5,-9.949039234562201E-5,-1.3743110966083036E-4,-1.220861850839077E-4,-7.270520456574008E-6,-4.794067494785702E-5,-1.6001649194372018E-4,-6.897265795091829E-5,-4.880236449426477E-5,1.3402233325957677E-5,5.767258606215922E-5,1.542563561809295E-4,-1.069253648751482E-5,4.6310307507798916E-4,5.064713160877908E-4,-2.7510849808122698E-5,-6.209905437502168E-7,4.116805419327233E-4,5.118299026634247E-4,-1.272095697628441E-5,-9.768586637571306E-5,4.035323801065597E-4,4.941752601041919E-4,-1.4003054494785716E-4,-8.453533802138454E-5,1.70134563428052E-4,4.4065539465695786E-4,-0.001113456726277516,-3.2967274892500685E-4,4.506276305028439E-4,-1.3613303409159728E-4,-0.001192033966626986,-9.385537010530525E-5])\n",
      "Intercept: 0.17492273063554667"
     ]
    },
    {
     "data": {
      "text/plain": [
       "logr = logreg_1cdef46b0f22\n",
       "logrModel = LogisticRegressionModel: uid = logreg_1cdef46b0f22, numClasses = 2, numFeatures = 692\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid = logreg_1cdef46b0f22, numClasses = 2, numFeatures = 692"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.classification.BinaryLogisticRegressionSummary\n",
    "\n",
    "val logr = new LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "\n",
    "val logrModel = logr.fit(trainingData)\n",
    "\n",
    "println(s\"Weights: ${logrModel.coefficients}\\nIntercept: ${logrModel.intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D@570008e3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trainingSummaryLR = org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummaryImpl@40d31421\n",
       "objectiveHistoryLR = Array(0.6857828960045463, 0.6691343074816642, 0.6246053611107611, 0.6167708962917943, 0.6098453745833217, 0.6070718712554954, 0.6034998568434863, 0.6010570229624692, 0.5971841188475281, 0.5959490971424294, 0.5946659626620877)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Array(0.6857828960045463, 0.6691343074816642, 0.6246053611107611, 0.6167708962917943, 0.6098453745833217, 0.6070718712554954, 0.6034998568434863, 0.6010570229624692, 0.5971841188475281, 0.5959490971424294, 0.5946659626620877)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainingSummaryLR = logrModel.summary\n",
    "val objectiveHistoryLR = trainingSummaryLR.objectiveHistory\n",
    "println(objectiveHistoryLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Least Squares\n",
    "\n",
    "- Most common formulation for regression problems\n",
    "- Average loss = Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "val lr = new LinearRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "\n",
    "val lrModel = lr.fit(trainingData)\n",
    "\n",
    "println(s\"Weights: ${lrModel.coefficients}\\nIntercept: ${lrModel.intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "val trainingSummaryLLS = lrModel.summary\n",
    "val objectiveHistoryLLS = trainingSummaryLLS.objectiveHistory\n",
    "\n",
    "println(objectiveHistoryLLS)\n",
    "\n",
    "trainingSummaryLLS.residuals.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "Having completed this lesson, you should be able to:\n",
    "\n",
    "- Understand the Pipelines API for Logistic Regression and Linear Least Squares\n",
    "- Perform classification with Logistic Regression\n",
    "- Perform classification with Linear Least Squares\n",
    "- Use regularization with Logistic Regression and Linear Least Squares\n",
    "\n",
    "### About the Authors\n",
    "\n",
    "[Petro Verkhogliad](https://www.linkedin.com/in/vpetro) is Consulting Manager at Lightbend. He holds a Masters degree in Computer Science with specialization in Intelligent Systems. He is passionate about functional programming and applications of AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
